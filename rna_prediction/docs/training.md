# Network training and execution

The implemented network can be executed through the `rnafolding.py` script.  

/usr/local/lib/python3.7/site-packages/lasagne/layers/pool.py
line 6 : from theano.tensor.signal.pool import pool_2d
## RNA folding

Trains a neural network using provided RNA secondary structures in JSON format.
The network's hyper parameters are provided using command line options.
Optionally, the predictions and other statistics are stored to a given
directory, where they can be analyzed further using the supplied plotting tools.
Furthermore, the network weights can be stored as well so that later on they
can be imported to perform further training or benchmarks.

Besides typical hyperparameters (e.g. learning rate), the architecture can also
be varied through parameters (e.g. enable LSTM bypass or weight sharing).

The network can be operated in both training and execution mode. For execution
mode, existing weights are imported and the network is applied to a given input.

Predictions and statistics are exported in the NumPy binary format (`.npy`).
The used parameters are stored in JSON format in the same folder.

The default parameters are set to reasonable values
for execution on a *NVIDIA GTX 1080TI* graphics card (11 GiB VRam).

The arguments are grouped into multiple categories.


### Miscellaneous

**-h**  
Shows help message, describing all parameters

**--training-data** TRAINING_FILE  
The set of sequences to train on, as generated by `prepare_data.py`.
If not provided, no training is performed.

**--validation-data** VALIDATION_FILE  
The set of sequences to perform validation on, as generated by `prepare_data.py`.
If not provided, training data is split into training and validation set based on 
the `val_ratio` argument.

**--test-data** TEST_FILE  
The set of sequences to test on.
Note: The architecture does NOT support training and testing during one run. One has to
first train/validate, THEN import weights and test on this test-data.

**--depth** DEPTH  
The number of conv or conv + LSTM blocks in the network  
Default: 5

**--init-weights** WEIGHTS_FILE  
File containing pre-defined weights to be initialize the network with.  
**Important:** The weights can only be used with the exact same network as they
were obtained in the first place. Especially, the convolutional and LSTM layer
arguments and the depth of the network must be the same. Parameters can be
retrieved from the `args.json` file which is supplied in each output folder.  
Default: None

**--maxlen** MAXLEN  
Maximum length of sequences that are considered in the training phase  
Default: 450

**--maxlen-validation** MAXLEN  
Maximum length of sequences that are considered in the validation phase   
Default: 450

**--mode** MODE  
The type of architecture used. 'lstm' for recurrent net, 'pool' for fully convolutional. 
Default: 'pool'


### Training

**--val-ratio** VAL_RATIO  
Validation ratio for the training data to be split into training and validation set.
Not used if validation data is explicitly given.   
Default: 0.2

**--epochs** NUM_EPOCHS  
The number of epochs to run through the training and validation set.

**--loss-func** {binary-crossentropy, squared-error, binary-hinge-loss}    
The loss function to be used during training, should be binary crossentropy.  
Default: binary-crossentropy

**--update-func** {sgd, momentum, nesterov, adam, rmsprop}  
The method of optimization. For methods with more parameters than only the
learning rate, the default lasagne values are used.  
Default: rmsprop

**--weight-decrease** WEIGHT_DECREASE  
In our architecture all intermediate outputs are considered. This factor decides
how strongly they are weighted. The last output is always weighted with one.
The previous outputs are weighted with WEIGHT_DECREASE ^ index.  
Default: 0.5


### Output

**--output-path** OUTPUT_PATH  
Path to write the output (predictions, weights, stats, parameters) to.  
Overwriting an existing path is not possible.  
If no path is supplied, the output is discarded.

**--output-predictions** OUTPUT_PREDICTIONS  
Lets the program store the predictions of every OUTPUT_PREDICTIONS-th epoch.  
Default: 5

**--output-weights** OUTPUT_WEIGHTS  
Lets the program store the network weights in every OUTPUT_PREDICTIONS-th epoch.  
Default: 1

**--threshold** THRESHOLD   
The threshold to binary classify the network output.
Values greater than *threshold* are considered positive,
smaller values are considered negative.  
Default: 0.5


### Convolution

**--filters** NUM_FILTERS  
Number of filters of the convolution in each block.  
Default: 16

**--filter-size** FILTER_SIZE  
Kernel size of the convolutions being performed  
Default: 11

**--filter-size-growth** FILTER_SIZE_GROWTH:  
Increase the size of the convolutional kernel by FILTER_SIZE_GROWTH
(additive) with each successive block.  
**This is not compatible with weight sharing!**  
Default: 0


#### Weight Sharing

Since weight-sharing requires the kernels of all sharing layers to be of the
same size and we want to increase the number of filters when sharing weights,
**weight sharing happens in three steps**.  

From block SHARE_WEIGHTS_FROM on, the number of filters is increased to
NUM_FILTERS_SHARED. The number of input channels is still NUM_FILTERS.
In block SHARE_WEIGHTS_FROM + 1, the number of input channels is now
NUM_FILTERS_SHARED. In each following layer, the weights from block
SHARE_WEIGHTS_FROM + 1 can be used.

**Weight sharing is only possible when filter-size-growth is set to 0.**

**--filters-shared** NUM_FILTERS_SHARED  
The number of filters of the convolutional layer from SHARE_WEIGHTS_FROM + 1 on.  
Default: 32

**--share-weights-from** SHARE_WEIGHTS_FROM  
The block from which to start sharing weights  
Default: 1000


### LSTM

**--use-lstm** {never, output, always, always-bypass}  
Define in which way LSTMs are to be used in the network  
* *never*: No LSTMs are used
* *output*: LSTMs are only used in the last block to generate the final output
* *always*: Use LSTMs in each block. The input for the convolution of a block is
only the network input and the LSTM output of the previous block.
* *always-bypass*: Use LSTM in each block, but bypass the convolution result
past the LSTM. The input for the convolution of a block is the network input,
previous conv output and previous lstm output.

Default: always-bypass

**--units**  
The number of units (output-channels) of the LSTMs.  
Default: 16
